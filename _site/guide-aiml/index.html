<p style="text-align:center;">
    Construct your prompt from the instructions below then use the E4S Guide Bot
</p>
<style>
/* E4S theme-adaptive pill button with white text */
.btn--e4s {
  background-color: var(--e4s-blue);
  color: #ffffff !important;
  border: none;
  border-radius: 9999px;
  padding: 0.6em 1.6em;
  font-weight: 600;
  display: inline-block;
  transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out;
  text-decoration: none;
}
.btn--e4s:hover,
.btn--e4s:focus {
  background-color: var(--e4s-blue-hover);
  color: #ffffff !important;
  text-decoration: none;
}
:root {
  --e4s-blue: #0092ca;
  --e4s-blue-hover: #006b99;
}
[data-theme="dark"] {
  --e4s-blue: #33b8f2;
  --e4s-blue-hover: #66ccff;
}
</style>

<p style="text-align:center;">
  <a href="https://chatgpt.com/g/g-69010fb294b0819195727cf1a7ccc792-software-ecosystem-for-science-guide" class="btn btn--e4s">Chat with the E4S Guide Bot</a>
</p>

<h2 id="introduction">Introduction</h2>

<p>Artificial Intelligence (AI) and Machine Learning (ML) are rapidly evolving areas that increasingly intersect with high-performance computing (HPC). Within the E4S ecosystem, AI/ML tools are selected and supported to provide scalable, portable, and sustainable foundations for scientific discovery. These tools range from industry-standard frameworks such as TensorFlow and PyTorch, to specialized scientific and workflow-oriented environments like DeepHyper, LBANN, and SmartSim.</p>

<p>Selecting the right AI/ML library or tool depends on understanding both the <strong>characteristics of your problem</strong> and the <strong>environment in which you will develop and run it</strong>. E4S provides a curated set of interoperable, performance-tuned AI/ML products, making it easier for researchers to combine familiar AI workflows with HPC architectures.</p>

<blockquote>
  <p><strong>Example:</strong>
I am training a deep neural network to emulate a climate simulation model.<br />
My data are multi-dimensional arrays stored in HDF5 format, generated from HPC simulations.<br />
The training will run on a large GPU-based supercomputer that uses NVIDIA A100 devices and MPI for distributed communication.<br />
I want to use mixed-precision training for better performance but need high numerical accuracy during validation.<br />
The model should be exportable to ONNX for inference on different systems.<br />
I also need to perform hyperparameter optimization across hundreds of nodes using the batch scheduler.<br />
Please suggest which AI/ML libraries or tools in E4S are best suited for this task, and explain why.</p>
</blockquote>

<p>The following tables outline attributes that can help a newcomer — or an automated assistant — reason about which AI/ML tools best fit a given use case. These attributes are divided into broadly meaningful attributes and those specific to certain situations.</p>

<hr />

<h2 id="broadly-meaningful-attributes">Broadly Meaningful Attributes</h2>

<table>
  <thead>
    <tr>
      <th>Attribute</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Primary goal</td>
      <td>The main purpose of the AI/ML task, such as training, inference, surrogate modeling, or reinforcement learning.</td>
    </tr>
    <tr>
      <td>Data modality</td>
      <td>The type of data used, such as image, text, tabular, time series, graph, or multi-modal combinations.</td>
    </tr>
    <tr>
      <td>Computational scale</td>
      <td>The size and complexity of the workload, ranging from single-node prototyping to large-scale distributed training across supercomputers.</td>
    </tr>
    <tr>
      <td>Hardware targets</td>
      <td>The intended hardware platform(s), such as CPU, NVIDIA GPU, AMD GPU, Intel GPU, or other accelerators.</td>
    </tr>
    <tr>
      <td>Precision requirements</td>
      <td>The numeric precision(s) used during training or inference (e.g., FP64, FP32, BF16, FP8) and support for mixed or adaptive precision.</td>
    </tr>
    <tr>
      <td>Framework interoperability</td>
      <td>Compatibility with major frameworks such as PyTorch, TensorFlow, JAX, or ONNX.</td>
    </tr>
    <tr>
      <td>HPC integration</td>
      <td>Availability of MPI, NCCL, RCCL, oneCCL, or other communication libraries for distributed computation.</td>
    </tr>
    <tr>
      <td>Portability</td>
      <td>The ability to run effectively on different architectures and compilers through abstractions like Kokkos or SYCL.</td>
    </tr>
    <tr>
      <td>Licensing and support model</td>
      <td>Type of license (e.g., open-source, permissive, copyleft) and level of community or vendor support.</td>
    </tr>
    <tr>
      <td>Maturity and adoption</td>
      <td>Stability, user base, and long-term support within the E4S or broader scientific community.</td>
    </tr>
    <tr>
      <td>Ease of use</td>
      <td>The learning curve and availability of documentation, examples, and APIs.</td>
    </tr>
    <tr>
      <td>Extensibility</td>
      <td>The ability to integrate custom operators, solvers, or domain-specific modules.</td>
    </tr>
    <tr>
      <td>Workflow integration</td>
      <td>Compatibility with workflow tools (e.g., SmartSim, DeepHyper, or MLFlow) and data pipelines in HPC environments.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="situation-specific-attributes">Situation-Specific Attributes</h2>

<h3 id="for-training-deep-neural-networks">For Training Deep Neural Networks</h3>

<table>
  <thead>
    <tr>
      <th>Attribute</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Parallelism model</td>
      <td>Supported training parallelism types: data, model, pipeline, or hybrid.</td>
    </tr>
    <tr>
      <td>Gradient synchronization</td>
      <td>Methods used for distributed optimization (e.g., AllReduce, parameter server, decentralized).</td>
    </tr>
    <tr>
      <td>Checkpointing</td>
      <td>Capabilities for saving and restoring training state efficiently at scale.</td>
    </tr>
    <tr>
      <td>Data loading</td>
      <td>Support for streaming or parallel I/O with HPC file systems.</td>
    </tr>
    <tr>
      <td>Mixed precision optimization</td>
      <td>Automatic handling of reduced-precision arithmetic for speed and memory efficiency.</td>
    </tr>
  </tbody>
</table>

<h3 id="for-inference-and-deployment">For Inference and Deployment</h3>

<table>
  <thead>
    <tr>
      <th>Attribute</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Latency sensitivity</td>
      <td>Acceptable inference delay (e.g., real-time, batch, or offline processing).</td>
    </tr>
    <tr>
      <td>Model format</td>
      <td>Supported model export and import standards (e.g., ONNX, SavedModel, TorchScript).</td>
    </tr>
    <tr>
      <td>Accelerator compatibility</td>
      <td>Ability to deploy on specialized inference hardware (e.g., TensorRT, Habana, Intel Gaudi).</td>
    </tr>
    <tr>
      <td>Scaling method</td>
      <td>Mechanism for parallel inference, replication, or sharding across compute nodes.</td>
    </tr>
    <tr>
      <td>Resource management</td>
      <td>Integration with schedulers and container runtimes such as Slurm, Kubernetes, or Singularity.</td>
    </tr>
  </tbody>
</table>

<h3 id="for-scientific-surrogate-modeling-or-emulation">For Scientific Surrogate Modeling or Emulation</h3>

<table>
  <thead>
    <tr>
      <th>Attribute</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Physics-informed capability</td>
      <td>Ability to incorporate physical constraints or governing equations (e.g., PINNs).</td>
    </tr>
    <tr>
      <td>Uncertainty quantification</td>
      <td>Support for probabilistic modeling or Bayesian inference.</td>
    </tr>
    <tr>
      <td>Integration with simulation data</td>
      <td>Native support for HDF5, ADIOS2, or custom data formats common in HPC.</td>
    </tr>
    <tr>
      <td>Surrogate training scalability</td>
      <td>Ability to scale to large training datasets from simulation output.</td>
    </tr>
    <tr>
      <td>Coupling with simulation codes</td>
      <td>APIs for embedding inference directly within simulation workflows.</td>
    </tr>
  </tbody>
</table>

<h3 id="for-hyperparameter-optimization-and-workflow-automation">For Hyperparameter Optimization and Workflow Automation</h3>

<table>
  <thead>
    <tr>
      <th>Attribute</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Search strategies</td>
      <td>Types of hyperparameter search supported (e.g., random, Bayesian, evolutionary).</td>
    </tr>
    <tr>
      <td>Scheduler awareness</td>
      <td>Integration with HPC schedulers for parallel job launches.</td>
    </tr>
    <tr>
      <td>Experiment tracking</td>
      <td>Built-in tools for tracking experiments, configurations, and results.</td>
    </tr>
    <tr>
      <td>Automation framework</td>
      <td>Compatibility with tools like DeepHyper, Ray Tune, or MLFlow.</td>
    </tr>
    <tr>
      <td>Reproducibility</td>
      <td>Mechanisms to ensure deterministic experiments and versioned configurations.</td>
    </tr>
  </tbody>
</table>

<h3 id="for-edge-or-hybrid-hpc-ai-environments">For Edge or Hybrid HPC-AI Environments</h3>

<table>
  <thead>
    <tr>
      <th>Attribute</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Resource heterogeneity</td>
      <td>Support for distributed execution across mixed CPU-GPU or edge-cloud systems.</td>
    </tr>
    <tr>
      <td>Model compression</td>
      <td>Ability to quantize or prune models for lightweight deployment.</td>
    </tr>
    <tr>
      <td>Data streaming</td>
      <td>Support for continuous data ingestion and inference pipelines.</td>
    </tr>
    <tr>
      <td>Connectivity requirements</td>
      <td>Handling of intermittent network connections or federated learning setups.</td>
    </tr>
    <tr>
      <td>Security and privacy</td>
      <td>Support for encrypted models, federated updates, or privacy-preserving training.</td>
    </tr>
  </tbody>
</table>
