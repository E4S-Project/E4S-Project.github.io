<p>The <strong>Ecosystem for Scientific Software (E4S)</strong> provides a curated collection of interoperable, performance-portable software packages that enable scalable, high-performance applications across diverse computing architectures. Within the E4S ecosystem, <strong>portable parallel programming systems</strong> play a critical role in achieving both productivity and performance on heterogeneous systems — including multicore CPUs, GPUs, and emerging accelerators — from vendors such as NVIDIA, AMD, and Intel.</p>

<p>E4S includes a set of portable programming models, frameworks, and runtime systems that allow application developers to write code once and execute efficiently across multiple platforms. These systems cover both <strong>intra-node (shared-memory and accelerator)</strong> parallelism and <strong>inter-node (distributed-memory)</strong> parallelism, while also leveraging modern <strong>language-supported features</strong> available in Fortran, C++, and LLVM-based compilers.</p>

<hr />

<h2 id="the-value-of-portable-programming-layers">The Value of Portable Programming Layers</h2>

<p>Portable programming layers abstract away hardware-specific details while maintaining high performance. This enables developers to target a wide range of architectures without rewriting large portions of their applications.</p>

<h3 id="mpi-the-backbone-of-distributed-scientific-computing">MPI: The Backbone of Distributed Scientific Computing</h3>
<p><strong>MPI (Message Passing Interface)</strong> remains the foundational model for distributed-memory programming. It provides a standardized API for exchanging messages among processes running on different nodes in a cluster. E4S supports <strong>MPICH</strong>, <strong>OpenMPI</strong>, and vendor-tuned variants (e.g., Cray MPICH), ensuring broad portability and performance across DOE supercomputers. MPI’s stability, mature tooling, and ecosystem-wide interoperability make it indispensable for scalable applications.</p>

<h3 id="kokkos-and-the-rise-of-performance-portability">Kokkos and the Rise of Performance Portability</h3>
<p><strong>Kokkos</strong>, originating from Sandia National Laboratories, provides a C++ programming model for writing performance-portable code targeting multiple backends such as CUDA, HIP, SYCL, and OpenMP. Kokkos is tightly integrated with Trilinos, PETSc, and other E4S math libraries, allowing applications to exploit GPUs and CPUs through a single, modern C++ abstraction layer. Its companion ecosystem includes <strong>Kokkos Kernels</strong> and <strong>Kokkos Tools</strong>, providing optimized linear algebra and performance instrumentation capabilities.</p>

<h3 id="openmp-openacc-and-llvm-compiler-advancements">OpenMP, OpenACC, and LLVM Compiler Advancements</h3>
<p>Compiler-supported parallelism models such as <strong>OpenMP</strong> and <strong>OpenACC</strong> enable incremental parallelization of legacy codes using pragma directives. Recent advances in <strong>LLVM-based compilers</strong> — including <strong>Clang/Flang</strong> for C++ and Fortran — provide robust support for GPU offloading, unified memory, and mixed-language interoperability. These tools complement E4S programming systems by lowering barriers for performance portability in both research and production settings.</p>

<hr />

<h2 id="intra-node-programming-systems">Intra-Node Programming Systems</h2>

<p>These systems focus on exploiting shared-memory and accelerator-level parallelism within a compute node. They provide APIs and abstractions for multi-core CPUs, GPUs, and other accelerators.</p>

<table>
  <thead>
    <tr>
      <th>System</th>
      <th>Description</th>
      <th>Key Features</th>
      <th>Supported Architectures</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Kokkos</td>
      <td>C++ performance-portable programming model</td>
      <td>Multi-backend (CUDA, HIP, SYCL, OpenMP), hierarchical parallelism, memory spaces</td>
      <td>NVIDIA, AMD, Intel GPUs; CPUs</td>
    </tr>
    <tr>
      <td>RAJA</td>
      <td>Loop-based C++ abstraction layer developed by LLNL</td>
      <td>Portable loop execution and memory management</td>
      <td>CUDA, HIP, OpenMP, TBB</td>
    </tr>
    <tr>
      <td>OpenMP (LLVM/Clang)</td>
      <td>Directive-based parallelism model integrated in LLVM</td>
      <td>Tasking, SIMD, GPU offload support</td>
      <td>CPUs and GPUs across vendors</td>
    </tr>
    <tr>
      <td>OpenACC</td>
      <td>Directive-based accelerator programming model</td>
      <td>High-level GPU offload without deep hardware knowledge</td>
      <td>NVIDIA GPUs, multicore CPUs</td>
    </tr>
    <tr>
      <td>Legion</td>
      <td>Task-based parallel runtime for data-centric applications</td>
      <td>Logical regions, dynamic scheduling</td>
      <td>Multicore and distributed hybrid systems</td>
    </tr>
    <tr>
      <td>UPC++</td>
      <td>Asynchronous PGAS (Partitioned Global Address Space) library for C++</td>
      <td>Remote memory access, futures, distributed objects</td>
      <td>Shared and distributed memory systems</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="inter-node-programming-systems">Inter-Node Programming Systems</h2>

<p>These systems enable scalable distributed-memory programming, coordinating computation and communication across multiple nodes.</p>

<table>
  <thead>
    <tr>
      <th>System</th>
      <th>Description</th>
      <th>Key Features</th>
      <th>Supported Architectures</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MPI (MPICH, OpenMPI)</td>
      <td>Standard message-passing library for distributed-memory systems</td>
      <td>Point-to-point, collective, one-sided, persistent communication</td>
      <td>CPU/GPU clusters</td>
    </tr>
    <tr>
      <td>Charm++</td>
      <td>Message-driven, migratable objects for parallel applications</td>
      <td>Adaptive load balancing, asynchronous execution</td>
      <td>Clusters, exascale systems</td>
    </tr>
    <tr>
      <td>HPX</td>
      <td>Asynchronous C++ runtime system</td>
      <td>Futures, active messages, fine-grained parallelism</td>
      <td>Multicore and distributed nodes</td>
    </tr>
    <tr>
      <td>PaRSEC</td>
      <td>DAG-based runtime for distributed task scheduling</td>
      <td>Dynamic dependency tracking</td>
      <td>Heterogeneous clusters</td>
    </tr>
    <tr>
      <td>Legion Realm Runtime</td>
      <td>Distributed execution layer of Legion</td>
      <td>Scalable data distribution and communication</td>
      <td>Hybrid systems</td>
    </tr>
    <tr>
      <td>GasNet-EX</td>
      <td>Communication layer for PGAS languages (UPC++, Chapel)</td>
      <td>High-performance low-level messaging</td>
      <td>Clusters, leadership-class systems</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="language-supported-parallelism-via-llvm-compilers">Language-Supported Parallelism via LLVM Compilers</h2>

<p>Modern Fortran and C++ compilers built on <strong>LLVM infrastructure</strong> (including Flang, Clang, and vendor derivatives) now provide direct language and directive-based access to parallelism, significantly narrowing the gap between compilers and specialized libraries.</p>

<table>
  <thead>
    <tr>
      <th>Language / Compiler</th>
      <th>Parallelism Model</th>
      <th>E4S Relevance</th>
      <th>Key Features</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Fortran (Flang / LLVM Flang)</td>
      <td>Coarrays, DO CONCURRENT, OpenMP</td>
      <td>Interoperates with MPI and math libraries</td>
      <td>Standard parallel features with GPU offloading</td>
    </tr>
    <tr>
      <td>C++ (Clang / LLVM)</td>
      <td>C++17/20 parallel STL, OpenMP, SYCL</td>
      <td>Enables integration with Kokkos, RAJA, HPX</td>
      <td>Unified parallel execution policies</td>
    </tr>
    <tr>
      <td>SYCL (via DPC++, hipSYCL)</td>
      <td>Single-source heterogeneous programming model</td>
      <td>Bridges HPC and AI kernels in E4S</td>
      <td>Cross-vendor GPU support, C++ templates</td>
    </tr>
    <tr>
      <td>LLVM/OpenACC</td>
      <td>Directive-based offload</td>
      <td>Complements Kokkos and OpenMP for incremental migration</td>
      <td>Portable performance on accelerators</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="summary">Summary</h2>

<p>E4S programming systems enable developers to write portable, high-performance applications capable of running efficiently across diverse architectures. By integrating low-level communication systems like <strong>MPI</strong>, node-level abstractions like <strong>Kokkos</strong> and <strong>RAJA</strong>, and modern compiler technologies supporting <strong>OpenMP</strong>, <strong>OpenACC</strong>, and <strong>SYCL</strong>, E4S ensures that scientific applications can evolve with hardware and remain sustainable over decades.</p>

<p>Together, these tools form the foundation of the DOE’s exascale software ecosystem — unifying productivity, portability, and performance across the world’s fastest computing platforms.</p>
